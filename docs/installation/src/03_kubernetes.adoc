=== Kubernetes

You can find all the deployment files in the link:https://github.com/spoud/kafka-cost-control/tree/master/deployment[deployment] folder. This folder use link:https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/[Kustomize] to simplify the deployment of multiple instances with some variations.

The kubernetes deployment is in two parts. One part is the *kafka control software* (processing, ui, dashboard, etc.) and the other part is the *kafka metric scrapper*. You may have multiple kafka metric scrapper deployment (one per kafka cluster for example), but you should need only one kafka cost control deployment.

==== Kafka metric scraper

This part will be responsible to scrape kafka for relevant metrics. Depending on what metrics you want to provide you will need a user with read access to kafka metric but also kafka admin client. Read permission is enough ! You don't need a user with write permission.

This documentation will assume that you use the `dev/` folder, but you can configure as much Kustomize folders as you want. The `dev/` folder is a good starting point if you have confluent cluster running.

Copy the environment sample file:
[,shell]
----
cd deployment/kafka-metric-scrapper/dev
cp .env.sample .env
vi .env
----
Edit the environment file with the correct output topic, endpoints and credentials.

Be sure to edit the *namespace* in the link:https://github.com/spoud/kafka-cost-control/tree/master/deployment/kafka-metric-scrapper/dev/kustomization.yaml[kustomization.yaml] file.

Deploy the dev environment using kubectl

[,shell]
----
cd /deployment/kafka-metric-scrapper
kubectl apply -k dev
----

Wait for the deployment to finish and check the output topic for metrics. You should receive new data every minute.

==== Kafka cost control

For this part we will deploy the kafka stream application that is responsible to enrich the metrics, TimescaleDB for storing the metrics, kafka connect instance to sink the metric into the database, a grafana dashboard and a simple UI to define prices and contexts.

This documentation will assume that you use the `dev/` folder, but you can configure as much Kustomize folders as you want.


Copy the environment sample file:
[,shell]
----
cd deployment/kafka-cost-control/app
cp .env.sample .env
vi .env
----
Edit the environment file with the correct credentials. The database password can be randomly generated. It will be used by kafka connect and grafana.

Be sure to edit the *namespace* in the link:https://github.com/spoud/kafka-cost-control/tree/master/deployment/kafka-cost-control/app/kustomization.yaml[kustomization.yaml] file.

You also may want to adapt the ingress files to use a proper hosts. You will need two hosts, one for grafana and one for the kafka cost control application.

Deploy the application using kubectl

[,shell]
----
cd /deployment/kafka-metric-scrapper
kubectl apply -k app
----

==== Helm Chart (Strimzi only)

If your Kafka cluster is managed by the Strimzi operators, you can use the provided Helm chart (under `helm/kcc-strimzi`) to deploy
Cost Control with all the required components (Telegraf, Aggregator, UI, TimescaleDB, Kafka Connect) as well as all the required
users and topics. The Helm chart still assumes that you already have a Schema Registry deployed.
See the relevant README file for more information.

A component that is special to the Strimzi deployment is the Context Operator.
This operator detects `KafkaUser` resources with a specific annotation and automatically creates
a corresponding `Context` in Cost Control. This allows you to manage your contexts directly from Kubernetes.
See the README in `strimzi-operator` for more information.
