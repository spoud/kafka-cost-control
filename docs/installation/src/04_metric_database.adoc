=== Metric database

For storing the metrics, we recommend using either an external time series database or, alternatively, the built-in DuckDB integration.

The advantage of the DuckDB integration is that it is easy to set up and does not require any additional downstream components (e.g. Kafka Connect and a time series database).
However, the DuckDB integration is not as scalable as a dedicated external database. It currently also only supports downloading an export of the metrics for some time period.
These exports can be downloaded in JSON and CSV format for offline analysis (e.g. in a Jupyter notebook).
Note that when the DuckDB integration is used, a stream of aggregated metrics is still sent to Kafka, so that it is possible to switch to using an external time series database at any time.

The time series database approach is more scalable and allows leveraging the full power of the database for querying and analyzing the metrics.
However, it also requires additional components to be set up, such as Kafka Connect and the database itself.
Feel free to choose the database that suits your needs, but be careful to choose one that is compatible with Kafka connect (or is otherwise capable of receiving data from Kafka) so you can easily transfer metrics from Kafka to your database.
In this example we will assume that you're using TimescaleDB because it's the one we provide Kubernetes manifest for.

In the following sections we will show you how to set up both the DuckDB integration and TimescaleDB.
Feel free to skip the section that does not apply to your use case.

==== DuckDB Integration

The DuckDB integration is disabled by default. To enable it, set the following environment variables in your Aggregator deployment:

[source,yaml]
----
      containers:
        - name: kafka-cost-control
          image: spoud/kafka-cost-control:latest
          env:
            # enable the DuckDB integration
            - name: CC_OLAP_ENABLED
              value: "true"
            # path in the container where the DuckDB database will be stored
            # if this is not set, the data will be stored in-memory (i.e. it will be lost when the container is restarted) 
            - name: CC_OLAP_DATABASE_URL
              value: "jdbc:duckdb:/home/jboss/kafka-stream/duckdb.db"
----

Once the integration is enabled, you can export collected metrics from the DuckDB database by using curl:

[source,shell]
----
curl -X GET http://localhost:8080/olap/export
# Get all metrics from the last 30 days in CSV format
curl -H "Accept: text/csv" http://localhost:8083/olap/export > out.csv
# Get all metrics from the last 30 days in JSON format
curl -H "Accept: application/json" http://localhost:8083/olap/export > out.json
# Get all metrics for all of February 2025 in CSV format
curl -H "Accept: text/csv" "http://localhost:8083/olap/export?fromDate=2025-02-01T00:00:00Z&toDate=2025-03-01T00:00:00Z" > out.csv
----

Note that when using the `fromDate` and `toDate` parameters, the times must be specified in UTC in the ISO 8601 format (e.g. `2025-02-01T00:00:00Z`).
Otherwise the API will return a 400 Bad Request error.

==== TimescaleDB Database Schema

Feel free to adapt the partition size to fit your needs. In this example we put 7 days but please follow the link:https://docs.timescale.com/use-timescale/latest/hypertables/about-hypertables/#best-practices-for-time-partitioning[TimescaleDB documentation] to choose the right partition size for your use case. We recommend a value between 7 days and 1 month. Note that the Helm and Kustomize templates already execute this script when the database is created.

[source,sql]
----
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;
CREATE TABLE "kafka_aggregated-table-friendly"
(
    "startTime"         TIMESTAMP        NOT NULL,
    "endTime"           TIMESTAMP        NOT NULL,
    "entityType"        VARCHAR          NOT NULL,
    "initialMetricName" VARCHAR          NOT NULL,
    "name"              VARCHAR          NOT NULL,
    "value"             DOUBLE PRECISION NOT NULL,
    "cost"              DOUBLE PRECISION NULL,
    "tags"              JSONB            NOT NULL,
    "context"           JSONB            NOT NULL,
    PRIMARY KEY ("startTime", "endTime", "entityType", "initialMetricName", "name")
);

SELECT create_hypertable('kafka_aggregated-table-friendly', by_range('startTime', INTERVAL '7 day'));
----

To prevent the database from being overwhelmed by the amount of data, we recommend creating a retention policy. In this example we will keep the data for 2 years:

[source,sql]
----
SELECT add_retention_policy('kafka_aggregated-table-friendly', INTERVAL '2 years');
----

if you want to run the scripts above manually, you can use the interactive cli.
[source,shell]
----
kubectl exec -it -n <namespace> timescaledb-0 -- psql -U postgres -d postgres
----
