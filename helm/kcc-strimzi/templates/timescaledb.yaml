{{ $postgresPw := randAlphaNum 10 }}
{{ $aggregatedTableFriendlyTopic := (include "aggregatedTableFriendlyTopic" .) }}
{{ $outputTable := printf "%s_aggregated_table_friendly" .Release.Name }}
{{ $caCert := (include "strimziCaCert" .) }}

kind: Secret
apiVersion: v1
data:
  POSTGRES_PASSWORD: {{ $postgresPw | b64enc | quote }}
metadata:
  labels:
  name: {{ .Release.Name }}-timescaledb-secret 
---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: timescaledb
  name: {{ .Release.Name }}-timescaledb
spec:
  ports:
    - port: 5432
      name: timescaledb
      targetPort: 5432
  selector:
    k8s-app: {{ .Release.Name }}-timescaledb
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Release.Name }}-timescaledb
spec:
  selector:
    matchLabels:
      k8s-app: {{ .Release.Name }}-timescaledb
  serviceName: "{{ .Release.Name }}-timescaledb"
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: {{ .Release.Name }}-timescaledb
        {{- with .Values.timescaledb.labels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      terminationGracePeriodSeconds: 10
      volumes:
        - name: tsdb-init
          configMap:
            name: {{ .Release.Name }}-timescaledb-init
        - name: tmp
          emptyDir: {}
        - name: postgresql
          emptyDir: {}
      securityContext:
        {{- .Values.timescaledb.podSecurityContext | toYaml | nindent 8 }}
      containers:
        - name: timescaledb
          image: {{ .Values.timescaledb.image }}
          securityContext:
            {{- .Values.timescaledb.securityContext | toYaml | nindent 12 }}
          imagePullPolicy: Always
          resources:
            {{- .Values.timescaledb.resources | toYaml | nindent 12 }}
          env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-timescaledb-secret
                  key: POSTGRES_PASSWORD
          ports:
            - containerPort: 5432
              name: timescaledb
          volumeMounts:
            - name: tsdb-init
              mountPath: /docker-entrypoint-initdb.d
            - name: timescaledb-data
              mountPath: /var/lib/postgresql
              readOnly: false
            - name: tmp
              mountPath: /tmp
            - name: postgresql
              mountPath: /var/run/postgresql
  volumeClaimTemplates:
    - metadata:
        name: timescaledb-data
      spec:
        {{ if .Values.timescaledb.volumeClaimTemplate.storageClassName }}
        storageClassName: {{ .Values.timescaledb.volumeClaimTemplate.storageClassName }}
        {{ end }}
        {{ if .Values.timescaledb.volumeClaimTemplate.accessModes }}
        accessModes:
          {{ .Values.timescaledb.volumeClaimTemplate.accessModes | toYaml | nindent 10 }}
        {{ else }}
        accessModes: [ "ReadWriteOnce" ]
        {{ end }}
        resources:
          requests:
            storage: {{ .Values.timescaledb.volumeClaimTemplate.resources.requests.storage }}
---
# DB init script
apiVersion: v1
data:
  init-db.sql: |+
    CREATE TABLE "{{ $outputTable }}"
    (
        "startTime"         TIMESTAMP        NOT NULL,
        "endTime"           TIMESTAMP        NOT NULL,
        "entityType"        VARCHAR          NOT NULL,
        "initialMetricName" VARCHAR          NOT NULL,
        "name"              VARCHAR          NOT NULL,
        "value"             DOUBLE PRECISION NOT NULL,
        "cost"              DOUBLE PRECISION NULL,
        "tags"              JSONB            NOT NULL,
        "context"           JSONB            NOT NULL,
        PRIMARY KEY ("startTime", "endTime", "entityType", "initialMetricName", "name")
    );

    -- todo: bad syntax
    -- SELECT create_hypertable('{{ $outputTable }}', by_range('startTime', INTERVAL '1 day'));

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: {{ .Release.Name }}-timescaledb-init
---
# Connect Stuff
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  annotations:
    spoud.io/kcc-context.application: kafka-cost-control
  labels:
    strimzi.io/cluster: {{ .Values.strimzi.clusterName }}
  name: {{ .Release.Name }}-connect
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
    - resource:
        type: group
        name: connect-{{ .Release.Name }}-tsdb-sink 
        patternType: literal
      operations:
        - All
      host: "*"
    - resource:
        type: group
        name: {{ .Release.Name }}-connect
        patternType: literal
      operations:
        - All
      host: "*"
    - host: "*"
      operations:
      - All
      resource:
        name: {{ .Release.Name }}-connect-
        patternType: prefix
        type: topic
    - host: "*"
      operations:
      - Read
      - Describe
      - DescribeConfigs
      resource:
        name: {{ $aggregatedTableFriendlyTopic }}
        patternType: literal
        type: topic
---

apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: {{ .Release.Name }}-connect
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  resources:
    {{- .Values.connect.resources | toYaml | nindent 4 }}
  image: {{ .Values.connect.image }}
  version: 3.8.0
  replicas: 1
  bootstrapServers: {{ .Values.strimzi.bootstrapServer }}
  template:
    pod:
      metadata:
        labels:
          {{- .Values.connect.labels | toYaml | nindent 10 }}
      securityContext:
        {{- .Values.connect.securityContext | toYaml | nindent 8 }}
  authentication:
    type: scram-sha-512
    username: {{ .Release.Name }}-connect
    passwordSecret:
      secretName: {{ .Release.Name }}-connect
      password: password
  tls:
    trustedCertificates:
      - secretName: {{ $caCert }}
        certificate: "ca.crt"
  config:
    group.id: {{ .Release.Name }}-connect
    offset.storage.topic: {{ .Release.Name }}-connect-cluster-offsets
    config.storage.topic: {{ .Release.Name }}-connect-cluster-configs
    status.storage.topic: {{ .Release.Name }}-connect-cluster-status
    # -1 means it will use the default replication factor configured in the broker
    config.storage.replication.factor: -1
    offset.storage.replication.factor: -1
    status.storage.replication.factor: -1
    internal.key.converter: org.apache.kafka.connect.storage.StringConverter
    internal.value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter: org.apache.kafka.connect.storage.StringConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
---
# To use the KafkaConnector resource, you have to first enable the connector operator using
# the strimzi.io/use-connector-resources annotation on the KafkaConnect custom resource.
# From Apache Kafka 3.1.1 and 3.2.0, you also have to add the FileStreamSourceConnector
# connector to the container image. You can do that using the kafka-connect-build.yaml example.
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: {{ .Release.Name }}-tsdb-sink
  labels:
    # The strimzi.io/cluster label identifies the KafkaConnect instance
    # in which to create this connector. That KafkaConnect instance
    # must have the strimzi.io/use-connector-resources annotation
    # set to true.
    strimzi.io/cluster: {{ .Release.Name }}-connect
spec:
  class: io.confluent.connect.jdbc.JdbcSinkConnector
  tasksMax: 1
  config: {
    "topics": "{{ $aggregatedTableFriendlyTopic }}",
    "connection.url": "jdbc:postgresql://{{ .Release.Name }}-timescaledb:5432/postgres?sslmode=disable",
    "connection.user": "postgres",
    "connection.password": "{{ $postgresPw }}",
    "insert.mode": "upsert",
    "auto.create": "false",
    "table.name.format": "{{ $outputTable }}",
    "pk.mode": "record_value",
    "pk.fields": "startTime,endTime,entityType,initialMetricName,name",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "{{ .Values.schemaRegistry.url }}",
    "value.converter.basic.auth.credentials.source": "USER_INFO",
    "value.converter.basic.auth.user.info": "schema-registry-user:schema-registry-password",
    "transforms": "flatten",
    "transforms.flatten.type": "org.apache.kafka.connect.transforms.Flatten$Value",
    "transforms.flatten.delimiter": "_"
  }
