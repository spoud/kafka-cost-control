{{ $postgresPw := randAlphaNum 10 }}

kind: Secret
apiVersion: v1
data:
  POSTGRES_PASSWORD: {{ $postgresPw | b64enc | quote }}
metadata:
  labels:
  name: {{ .Release.Name }}-timescaledb-secret 
---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: timescaledb
  name: {{ .Release.Name }}-timescaledb
spec:
  ports:
    - port: 5432
      name: timescaledb
      targetPort: 5432
  selector:
    k8s-app: {{ .Release.Name }}-timescaledb
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Release.Name }}-timescaledb
spec:
  selector:
    matchLabels:
      k8s-app: {{ .Release.Name }}-timescaledb
  serviceName: "{{ .Release.Name }}-timescaledb"
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: {{ .Release.Name }}-timescaledb
    spec:
      terminationGracePeriodSeconds: 10
      initContainers:
        - name: pgsql-data-permission-fix
          image: busybox
          command: [ "/bin/chmod","-R","777", "/data" ]
          volumeMounts:
            - name: timescaledb-data
              mountPath: /data
        - name: pgsql-lost-and-found-fix
          image: busybox
          command: [ "/bin/rm", "-Rf", "/data/lost+found", "||", "echo", "done" ]
          volumeMounts:
            - name: timescaledb-data
              mountPath: /data
      containers:
        - name: timescaledb
          image: timescale/timescaledb:latest-pg16
          imagePullPolicy: Always
          resources:
            limits:
              cpu: 4000m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 0.5Gi
          env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-timescaledb-secret
                  key: POSTGRES_PASSWORD
          ports:
            - containerPort: 5432
              name: timescaledb
          volumeMounts:
            - name: timescaledb-data
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: timescaledb-data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 10Gi
---
# Connect Stuff
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  annotations:
    spoud.io/kcc-context.application: kafka-cost-control
  labels:
    strimzi.io/cluster: {{ .Values.strimzi.clusterName }}
  name: {{ .Release.Name }}-connect
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
    - resource:
        type: group
        name: {{ .Release.Name }}-connect
        patternType: literal
      operations:
        - All
      host: "*"
    - host: "*"
      operations:
      - All
      resource:
        name: {{ .Release.Name }}-connect-
        patternType: prefix
        type: topic
    # TODO: restrict this rule to only allow reading from the "...-aggregated-table-friendly" topic
    - host: "*"
      operations:
      - All
      resource:
        name: {{ .Release.Name }}-
        patternType: prefix
        type: topic
---

# apiVersion: kafka.strimzi.io/v1beta2
# kind: KafkaConnect
# metadata:
#   name: {{ .Release.Name }}-connect
#   annotations:
#     strimzi.io/use-connector-resources: "true"
# spec:
#   version: 3.8.0
#   replicas: 1
#   bootstrapServers: {{ .Values.strimzi.bootstrapServer }}
#   #image: spoud/kafka-cost-control-connect:latest
#   authentication:
#     type: scram-sha-512
#     username: {{ .Release.Name }}-connect
#     passwordSecret:
#       secretName: {{ .Release.Name }}-connect
#       password: password
#   #tls:
#   #  trustedCertificates:
#   #    - secretName: {{ .Values.strimzi.clusterName }}-cluster-ca-cert
#   #      pattern: "*.crt"
#   config:
#     group.id: {{ .Release.Name }}-connect
#     offset.storage.topic: {{ .Release.Name }}-connect-cluster-offsets
#     config.storage.topic: {{ .Release.Name }}-connect-cluster-configs
#     status.storage.topic: {{ .Release.Name }}-connect-cluster-status
#     # -1 means it will use the default replication factor configured in the broker
#     config.storage.replication.factor: -1
#     offset.storage.replication.factor: -1
#     status.storage.replication.factor: -1
# ---
# # To use the KafkaConnector resource, you have to first enable the connector operator using
# # the strimzi.io/use-connector-resources annotation on the KafkaConnect custom resource.
# # From Apache Kafka 3.1.1 and 3.2.0, you also have to add the FileStreamSourceConnector
# # connector to the container image. You can do that using the kafka-connect-build.yaml example.
# apiVersion: kafka.strimzi.io/v1beta2
# kind: KafkaConnector
# metadata:
#   name: {{ .Release.Name }}-tsdb-sink
#   labels:
#     # The strimzi.io/cluster label identifies the KafkaConnect instance
#     # in which to create this connector. That KafkaConnect instance
#     # must have the strimzi.io/use-connector-resources annotation
#     # set to true.
#     strimzi.io/cluster: {{ .Release.Name }}-connect
# spec:
#   class: io.confluent.connect.jdbc.JdbcSinkConnector
#   tasksMax: 2
#   # TODO: dynamically set the topics to be consumed by the connector
#   config: {
#     "topics": "{{ .Release.Name }}-aggregated-table-friendly",
#     "connection.url": "jdbc:postgresql://{{ .Release.Name }}-timescaledb:5432/postgres?sslmode=disable",
#     "connection.user": "postgres",
#     "connection.password": "{{ $postgresPw }}",
#     "insert.mode": "upsert",
#     "auto.create": "false",
#     "table.name.format": "kafka_${topic}",
#     "pk.mode": "record_value",
#     "pk.fields": "startTime,endTime,entityType,initialMetricName,name",
#     "key.converter": "org.apache.kafka.connect.storage.StringConverter",
#     "value.converter": "io.confluent.connect.avro.AvroConverter",
#     "value.converter.schema.registry.url": "{{ .Values.schemaRegistry.url }}",
#     "value.converter.basic.auth.credentials.source": "USER_INFO",
#     "value.converter.basic.auth.user.info": "schema-registry-user:schema-registry-password",
#     "transforms": "flatten",
#     "transforms.flatten.type": "org.apache.kafka.connect.transforms.Flatten$Value",
#     "transforms.flatten.delimiter": "_"
#   }

# TODO: use the above KafkaConnector resource instead of the below Deployment and Service resources
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-connect
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: {{ .Release.Name }}-kafka-connect
  template:
    metadata:
      labels:
        k8s-app: {{ .Release.Name }}-kafka-connect
    spec:
      containers:
        - name: kafka-connect
          image: spoud/kafka-cost-control-connect:latest
          imagePullPolicy: Always
          resources:
            limits:
              cpu: 2000m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 500Mi
          env:
            - name: CONNECT_REST_ADVERTISED_HOST_NAME
              value: kafka-connect
            - name: CONNECT_REST_PORT
              value: "8083"
            - name: CONNECT_GROUP_ID
              value: {{ .Release.Name }}-connect
            - name: CONNECT_INTERNAL_KEY_CONVERTER
              value: org.apache.kafka.connect.storage.StringConverter
            - name: CONNECT_INTERNAL_VALUE_CONVERTER
              value: org.apache.kafka.connect.json.JsonConverter
            - name: CONNECT_CONFIG_STORAGE_PARTITIONS
              value: "1"
            - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR
              value: "3"
            - name: CONNECT_CONFIG_STORAGE_TOPIC
              value: {{ .Release.Name }}-connect-configs
            - name: CONNECT_OFFSET_STORAGE_PARTITIONS
              value: "1"
            - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR
              value: "3"
            - name: CONNECT_OFFSET_STORAGE_TOPIC
              value: {{ .Release.Name }}-connect-offsets
            - name: CONNECT_STATUS_STORAGE_PARTITIONS
              value: "1"
            - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR
              value: "3"
            - name: CONNECT_STATUS_STORAGE_TOPIC
              value: {{ .Release.Name }}-connect-status
            - name: CONNECT_KEY_CONVERTER
              value: org.apache.kafka.connect.storage.StringConverter
            - name: CONNECT_VALUE_CONVERTER
              value: org.apache.kafka.connect.json.JsonConverter

            # Connect username and password
            - name: CLUSTER_API_KEY
              value: "{{ .Release.Name }}-connect"
            - name: CLUSTER_API_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-connect
                  key: password

            # Connect main connection
            - name: CONNECT_BOOTSTRAP_SERVERS
              value: {{ .Values.strimzi.bootstrapServer }}
            - name: CONNECT_SASL_JAAS_CONFIG
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-connect
                  key: sasl.jaas.config
            - name: CONNECT_SECURITY_PROTOCOL
              value: SASL_PLAINTEXT
            - name: CONNECT_SASL_MECHANISM
              value: SCRAM-SHA-512

            # Connect consumer connection
            - name: CONNECT_CONSUMER_BOOTSTRAP_SERVERS
              value: "$(CONNECT_BOOTSTRAP_SERVERS)"
            - name: CONNECT_CONSUMER_SASL_JAAS_CONFIG
              value: "$(CONNECT_SASL_JAAS_CONFIG)"
            - name: CONNECT_CONSUMER_SECURITY_PROTOCOL
              value: "$(CONNECT_SECURITY_PROTOCOL)"
            - name: CONNECT_CONSUMER_SASL_MECHANISM
              value: "$(CONNECT_SASL_MECHANISM)"

            # Connect producer connection
            - name: CONNECT_PRODUCER_BOOTSTRAP_SERVERS
              value: "$(CONNECT_BOOTSTRAP_SERVERS)"
            - name: CONNECT_PRODUCER_SASL_JAAS_CONFIG
              value: "$(CONNECT_SASL_JAAS_CONFIG)"
            - name: CONNECT_PRODUCER_SECURITY_PROTOCOL
              value: "$(CONNECT_SECURITY_PROTOCOL)"
            - name: CONNECT_PRODUCER_SASL_MECHANISM
              value: "$(CONNECT_SASL_MECHANISM)"
          ports:
            - containerPort: 8083
              name: http
  strategy:
    type: Recreate
  revisionHistoryLimit: 2
  progressDeadlineSeconds: 60

---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: {{ .Release.Name }}-kafka-connect
  name: {{ .Release.Name }}-kafka-connect-service
spec:
  ports:
    - port: 8083
      name: http
      targetPort: 8083
  selector:
    k8s-app: {{ .Release.Name }}-kafka-connect
