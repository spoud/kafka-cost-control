
# Default values for Kafka Cost Control.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Used as a tag in all scraped metrics. Helps you to identify the environment where the metrics are coming from.
env: dev
# Set this to true to enable debug logging in telegraf and the context operator
debug: true

topics:
  context: {} # when unset, the below values are generated from the release name
    #topicName: "context-data"
  rawMetrics:
    #topicName: "raw-metrics" # if unset, the topic name is generated from the release name
    partitions: 3
    # default: 90 days retention
    retentionMs: "7776000000"
  pricingRules:
    #topicName: "pricing-rules" # if unset, the topic name is generated from the release name
    partitions: 1
  aggregated: # TODO: read this from global values
    #topicName: "aggregated" # if unset, the topic name is generated from the release name
    partitions: 1
    config:
      retentionMs: "7776000000"
  aggregatedTableFriendly: # TODO: read this from global values
    #topicName: "aggregated-table-friendly" # if unset, the topic name is generated from the release name
    partitions: 1
    config:
      retentionMs: "7776000000"
  # list of raw metrics topics that should be consumed by the aggregator.
  # If empty, the aggregator will consume the topic defined in the rawMetrics section
  toAggregate: []

telegraf:
  labels: {}
  resources:
    limits:
      cpu: 1
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 100Mi

strimzi:
  clusterName: my-cluster
  bootstrapServer: my-cluster-kafka-bootstrap:9093
  # TODO: actually add support for tls and none
  # "scram-sha-512" or "tls" or "none"
  auth: scram-sha-512
  scramOverTls: true
  contextOperator:
    labels: {}
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 5
    readinessProbe:
      initialDelaySeconds: 15
      periodSeconds: 10
      failureThreshold: 5
    startupProbe:
      initialDelaySeconds: 15
      periodSeconds: 10
      failureThreshold: 10
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi

# Don't have a schema registry and would like to try out this chart? Run the following two commands:
# kubectl create deploy schema-registry --image=apicurio/apicurio-registry:3.0.5 -n <your strimzi namespace> --port 8080
# kubectl expose deploy schema-registry
schemaRegistry:
  url: http://schema-registry:8080/apis/ccompat/v7

aggregator:
  # TODO: actually don't spin up the aggregator if this is false
  enabled: true
  appId: kcc-aggregator
  # Labels that will be added to the pod
  labels: {}
  resources:
    limits:
      cpu: 4000m
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 1Gi
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

connect:
  # TODO: build and use a spoud image (cannot be built on top of a confluent image, because strimzi does not know how to start it)
  image: alero00/kafka-connect-jdbc-sink:latest
  # TODO: actually don't spin up the connect and timescale if this is false
  enabled: true
